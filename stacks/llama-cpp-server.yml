networks:
  network:
    name: ${NETWORK:-localnet}
    external: true

services:
  llama-cpp-server:
    image: ${IMAGE:-ghcr.io/ggml-org/llama.cpp:server-vulkan}
    # if using full image, use the following to auto start server
    #command: ["--server","--model","/app/models/12B/gemma-3-12b-it-q4_0.gguf","-c","2048","-ngl","43","-mg","1","--port","80"]
    # if using full image, use the following to auto start a benchmark.
    #command: ["--bench","--model","/app/models/12B/gemma-3-12b-it-q4_0.gguf","--device","Vulkan1"]
    hostname: ${HOSTNAME:-llama-cpp}
    domainname: ${DOMAINNAME:-localhost}
    devices:
      - /dev/kfd:/dev/kfd
      - /dev/dri:/dev/dri
    security_opt:
      - seccomp:unconfined
    volumes:
      - /opt/amdgpu:/opt/amdgpu:ro
      - /opt/gguf:/app/models
    networks:
      network:
    environment:
      - TZ=${TZ:-Australia/Brisbane}
      - LLAMA_ARG_MODEL=${LLAMA_ARG_MODEL:-/app/models/12B/gemma-3-12b-it-q4_0.gguf}
      - LLAMA_ARG_HOST=${LLAMA_ARG_HOST:-0.0.0.0}
      - LLAMA_ARG_PORT=${LLAMA_ARG_PORT:-80}
      # Set this to explicitly limit which device/s llama.cpp will use, e.g. Vulkan1,Vulkan2 to ignore Vulkan0 (e.g. iGPU) but use PCIe GPU #1 and PCIe GPU #2
      #- LLAMA_ARG_DEVICE=${LLAMA_ARG_DEVICE:-Vulkan0}
      # set this to 1 to prefer host memory instead of VRAM
      # - GGML_VK_PREFER_HOST_MEMORY=${GGML_VK_PREFER_HOST_MEMORY:-0}
    restart: unless-stopped
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.llama_cpp.rule=Host(`${HOSTNAME:-llama-cpp}.${DOMAINNAME:-localhost}`)"
      - "traefik.http.services.llama_cpp.loadbalancer.server.port=${LLAMA_ARG_PORT:-80}"
      - "com.centurylinklabs.watchtower.enable=true"
      - "wud.watch=true"
      - "io.containers.autoupdate=registry"
